---
output:
  html_document:
    toc: true
    number_sections: false
    toc_depth: 3
author: "Carl Beuchel"
date: "`r format(Sys.time(), '%d %B, %Y')`"
editor_options: 
  chunk_output_type: console
---

```{r knitr, cache = F, results = "hide", echo = F ,include = T}
# start with a clean workspace
rm(list=ls())
gc()

# Set the global knitr options
knitr::opts_chunk$set(cache = F, results = "hide", echo = F ,include = T, warning = F, message = F)

# what should the output file be called?
filename = "STEP 2: Zero-inflated data exploration"
```

# `r filename`
***
This script is a small data simulation to help figure out an appropriate data transformation withouth loosing too much statistical power in the subsequent regression analyses.

```{r initiate, message=FALSE, warning=T}
# choose correct working directory
r_on_server <- T
if (r_on_server == T) {
  basicpath <- "/net/ifs1/san_projekte/projekte/"
} else {
  basicpath <- "/mnt/ifs1_projekte/"
}
setwd(basicpath)

# set working directory
pathwd <-
  paste0(
    basicpath,
    "genstat/02_projekte/1703_ge_metab_a1_b3_sorbs/171124_ZeroInflatedDataSim"
  )
setwd(pathwd)

# get additional functions from Holger Kirstens newest RProfile file
newest_rprofile <-
  function(designation = paste0(basicpath, "genstat/07_programme/rtools/RProfile_hk/")) {
    files <- list.files(designation)
    RProfiles <- files[grep("Rprofile_hk_", files)]
    newest_RProfile <- tail(sort(RProfiles), n = 1)
    suppressPackageStartupMessages(source(paste0(designation, newest_RProfile)))
  }
newest_rprofile()

# start time measurement, define alternative package directory, define start codes for external start of the script
if (r_on_server == F) {
  initializeSkript(myfilename = filename, computer = "forostar") # enter which server you are on
} else
  initializeSkript(myfilename = filename, computer = "local")

# Packages
for (i in c(
  "knitr",
  "data.table",
  "MASS",
  "ggplot2",
  "scales"
)) {
  suppressPackageStartupMessages(library(i, character.only = TRUE))
}
```

## Data simulation and setup

```{r load}
# load the simuation results
all.scenarios <- fread("/mnt/ifs1_projekte/genstat/02_projekte/1703_ge_metab_a1_b3_sorbs/171124_ZeroInflatedDataSim/results/171207_SimulationResults.csv")
```

### Additional Functions

```{r add.func}
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

# create a -log10 transformation for scales
minuslog10_trans <- function() trans_new("minuslog10", function(x) -log10(x), function(x) 10(-x))

# theme from 
my_theme = theme_minimal() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        axis.line = element_line(),
        text = element_text(family = 'Times'),
        legend.position = 'right')

```

## Exploratory plots

There are a few descisions that I need to make:
 - Bereiche or Quantile?
 - Best number of categories?
 - Performance compared with dichotomized glm?
 - Performance compared with Rank correlation?

### Performance of dichotomized metabolites vs. different number of categories in proportional odds model

Our best guess was, that grouping the metabolites into more categories than just 2 would result in the retaining of information and thus statistical power. These plots are supposed to illustrate the comparison between dichotomization and different versions of the proportional odds approach.

For the proportional odds model, the number of categories (`categnum`) and the way the categories were established (`categart`) vary. In this simulation, `r unique(all.scenarios$categnum)` were the number of categories tested.

For the data simulation, the parameters were the percentage of inflated zero-values (`proz0`) and the effekt size (`effekt`) varied. For each scenario, 5000 sampels were drawn. Each scenario was computed 1000 times. 

#### Dichotomization vs. proportional odds model

First it needs to be established, whether the proportional odds approach performs better than dichotomization. Intuitively it should, but we can test this by plotting the p-values of the approaches against each other and observe their properties depending on the different parameters. 

This first plot, plotting all p-values only shows, that the the percentage of zero-inflated values has a high impact on the distribution depending on the effect size. For small effect-sizes, which are the most interesting for our study, a distinction seems difficult. However, the plot let's us hypothesize, that that the proportional odds model outperforms the dichotomization model with increasing effect-size and a lower percentage of zero-inflated values. 

```{r question.1.1}
# First question: Dicho vs Logit
# plot all dicho vs. prop odds pvals
p1 <- ggplot(all.scenarios, aes(-log10(dicho.pval),
                                -log10(logit.pval),
                                col = factor(proz0),
                                pch = categart,
                                size = categnum,
                                alpha = rev(categnum))) + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1, lty = 2) + 
  facet_wrap(~effekt , scales = "free") + 
  scale_y_continuous(breaks = pretty_breaks(4)) +
  scale_x_continuous(breaks = pretty_breaks(4)) +
  my_theme

# plot dicho vs logit pvals
p1
```

Taking the average p-values over all realizations provides a clearer picture for the differences for small effect-sizes. The proportional odds approach clearly outperforms dichotomization. Only when the zero-inflation is as high as 80%, the performances are similar. The table shows the sum of p-values for each approach <= 0.05 for each combination of number of categories and type of category.

```{r question.1.2}
# plot mean (over realizations) dicho vs. prop odds pvals
# über realisationen mitteln für Übersichtlichkeit
test.1 <- all.scenarios[, lapply(.SD, mean), by = .(effekt, proz0, categart, categnum)]
p2 <- ggplot(test.1, aes(x = -log10(dicho.pval),
                         y = -log10(logit.pval),
                         col = factor(proz0),
                         pch = categart,
                         size = categnum,
                         alpha = rev(categnum))) + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1, lty = 2) + 
  facet_wrap(~effekt , scales = "free") + 
  scale_y_continuous(breaks = pretty_breaks(4)) +
  scale_x_continuous(breaks = pretty_breaks(4)) +
  my_theme

# plot
# p2

# same as p2 but with different symbols etc.
p2.5 <- ggplot(test.1, aes(-log10(dicho.pval),
                   -log10(logit.pval),
                   col = categnum,
                   pch = categart,
                   size = factor(proz0),
                   alpha = rev(factor(proz0)))) +
  geom_point() +
  facet_wrap(~effekt, scales = "free") +
  scale_x_continuous(breaks = pretty_breaks(4)) +
  scale_y_continuous(breaks = pretty_breaks(4)) +
  geom_abline(intercept = 0, slope = 1, lty = 2) +
  scale_color_distiller(palette="Spectral") +
  scale_alpha_manual(values = c(0.3, 0.5, 0.9)) + 
  my_theme
  
p2.5

# number of categories as deciding factor
kable(all.scenarios[ , lapply(.SD, function(x) sum(x <= 0.05)), by = .(categart,categnum), .SDcols = c("linmod.pval", "logit.pval", "dicho.pval")])
```

Additionally we look at the performance of the prop-odds model against a linear model approach. We can see that, even though the normality assumtion of the linear model is violated, the linear model outperforms the prop-odds approach, increasing with percentage of zeor-inflation. 

```{r question.1.3}
# plot mean (over realizations) linmod vs. prop odds pvals
# how does logit model compare to linmod?
p3 <- ggplot(test.1, aes(x = -log10(linmod.pval),
                         y = -log10(logit.pval),
                         col = factor(proz0),
                         pch = categart,
                         size = categnum,
                         alpha = rev(categnum))) + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1, lty = 2) + 
  facet_wrap(~effekt , scales = "free") + 
  my_theme
p3
```

#### Optimum for number of categories

Plotting the p-values against the number of categories reveals a maximum towards a higher number of categories. However, the slope might be distorted due to the higher amount of different categories <10. Looking at the categories >10 only reveals no increased model performance when the number of categories exeeds 10. Again, the interpretation of the results for small effect sizes proves difficult, but the data suggests no increasded power upwards of 10 categories. 

```{r question.2}
# second question: what number of categories is optimal?
# plot -log10(Pwert) against number of categories
p4 <- ggplot(test.1,
             aes(x = categnum,
                 y = -log10(logit.pval),
                 pch = categart,
                 color = factor(proz0))) +
  facet_wrap(~ effekt, scale = "free") +
  geom_smooth(method = "loess", aes(group = paste(factor(proz0), categart), lty = categart), alpha = 0.2, fullrange=T, span = 1.5) +
  geom_point(size = 2.5) +
  scale_x_continuous(breaks = pretty_breaks(4)) +
  scale_y_continuous(breaks = pretty_breaks(4)) +
  scale_color_brewer(palette = "Set1") +
  my_theme

# all categnum
p4

# large categories
p4.1 <- ggplot(test.1[categnum %in% c(10,20,30,40,50),],
             aes(x = categnum,
                 y = -log10(logit.pval),
                 pch = categart,
                 color = factor(proz0))) +
  facet_wrap(~ effekt, scale = "free") +
  geom_smooth(method = "loess", aes(group = paste(factor(proz0), categart), lty = categart), alpha = 0.2, fullrange = T, span = 1.5) +
  geom_point(size = 2.5) +
  scale_x_continuous(breaks = pretty_breaks(4)) +
  scale_y_continuous(breaks = pretty_breaks(4)) +
  scale_color_brewer(palette = "Set1") +
  my_theme

# only look at higher number of categories
p4.1

# look at categories 2:10
p4.2 <- ggplot(test.1[categnum %in% 2:10,],
             aes(x = categnum,
                 y = -log10(logit.pval),
                 pch = categart,
                 color = factor(proz0))) +
  facet_wrap(~ effekt, scale = "free") +
  geom_smooth(method = "loess", aes(group = paste(factor(proz0), categart), lty = categart), alpha = 0.2, fullrange = T, span = 1.5) +
  geom_point(size = 2.5) +
  scale_x_continuous(breaks = pretty_breaks(4)) +
  scale_y_continuous(breaks = pretty_breaks(4)) +
  scale_color_brewer(palette = "Set1") +
  my_theme

# only look at higher number of categories
p4.2
```

#### Categories based on quantiles or range

The previous plots suggest that the categories based on quantiles perform better in terms of mean p-value for each category, which is again dependent on effect size and percentage of zero-inflation. Plotting the, averaged over all realizations of the model, -log10 p-values against each other reveal a slight advantage of quantile-defined categories, at least for larger effect sizes. For the effect sizes of 0.01 and 0.02 in 80% zero inflated data, no conclusion can be drawn from the data. However, the general trend of the simulation suggests a slight advantage of quantiles over ranges for subdividing the data into categories  

```{r question.3}
# third question: Bereiche oder Quantile
# plot bereiche ~ quantile pvals
test.2 <- dcast.data.table(data = test.1, formula = effekt + proz0 + categnum ~ categart, value.var = "logit.pval")
p5 <- ggplot(test.2,
       aes(x = -log10(bereiche),
           y = -log10(quantile),
           color = categnum,
           size = as.factor(proz0))) +
  facet_wrap(~effekt, scale = "free") + 
  geom_point(alpha = 1) +
  geom_abline(intercept = 0, slope = 1) + 
  scale_color_distiller(palette="Spectral") + 
  scale_alpha_manual(values = c(0.3,0.9))

p5
```

#### Comparison with Pearson's Rank Correlation

Comparing the results of the prop-odds model with rank correlation results show the advantage of rank-correlation, especially when the zero-inflation increases. The boxplots compare the p-value distribution of all approaches.  

```{r question.4}
# comparison to rank
# scatterplot
p6 <- ggplot(test.1, aes(x = -log10(pearson.cor.pval),
                         y = -log10(logit.pval),
                         col = factor(proz0),
                         pch = categart,
                         size = categnum,
                         alpha = rev(categnum))) + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1, lty = 2) + 
  scale_color_brewer(palette="Set1") + 
  scale_alpha_continuous(range = c(0.3, 1)) +
  facet_wrap(~effekt , scales = "free") + 
  my_theme

p6

# boxplot
test.3 <- melt(data = all.scenarios,
               id.vars = c("effekt", "proz0", "categart", "categnum"),
               measure.vars = c("logit.pval", "dicho.pval", "linmod.pval", "pearson.cor.pval"))

# outer loop for proz0
lapply(unique(all.scenarios$proz0), function(y){
  
  # inner loop for effect size
  lapply(unique(all.scenarios$effekt), function(x){
    
    p7 <- ggplot(test.3[effekt == x & proz0 == y, ],
                 aes(y = -log10(value),
                     x = as.factor(categnum),
                     fill = variable)) +
      geom_boxplot() + 
      coord_flip() +
      facet_wrap( ~ categart) + 
      ggtitle(label = "Boxplot of Pvalues",
              subtitle = paste0("Effect-size = ", x, " and zero-inflation by ", y, "%")) +
      theme_minimal() +
      scale_fill_brewer(palette = "Spectral")
    
    p7
  })
})
```

#### Comparison of effect sizes between linear model and proportional odds model

Plotting the average effect sizes from the linear model and the prop-odds model reveal a drastic overestimation of effect size with increasing effect size in the data by the prop-odds model. 

```{r question.5}
# define
p8 <- ggplot(test.1,
       aes(x = linmod.beta,
           y = logit.beta,
           size = categnum,
           col = factor(effekt),
           pch = categart,
           alpha = rev(factor(proz0)))) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, lty = 2, col = "black") +
  # facet_wrap( ~ , scales = "fixed") +
  scale_color_brewer(palette = "Set1") +
  scale_x_continuous(breaks = pretty_breaks(6)) +
  scale_y_continuous(breaks = pretty_breaks(9)) +
  scale_alpha_manual(values = c(0.3, 0.5, 0.9)) + 
  my_theme

# plot
p8

```

```{r save}
# write.table(x = all.scenarios,
#             file = paste0(pathwd, "/results/",
#                           format(Sys.time(), '%y%m%d'),
#                           "_SimulationResults.csv"),
#             sep = "\t", row.names = F)
```

```{r SessionInfo, echo=F, results='markup'}
sessionInfo()
```            