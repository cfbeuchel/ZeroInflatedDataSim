---
output:
  html_document:
    toc: true
    toc_float: 
      collapsed: false
      smooth_scroll: false
    number_sections: true
    toc_depth: 4
author: "Carl Beuchel"
date: "`r format(Sys.time(), '%d %B, %Y')`"
editor_options: 
  chunk_output_type: console
---

```{r knitr, cache = F, results = "hide", echo = F, warning = T}
# start with a clean workspace
rm(list=ls())
gc()

# Set the global knitr options
knitr::opts_chunk$set(cache = F, results = "hide", echo = F, include = T, message = T, warning = F)

# what should the output file be called?
filename = "STEP 2: Zero-inflated data exploration"
```

# `r filename`
***
This script is a small data simulation to help figure out an appropriate data transformation withouth loosing too much statistical power in the subsequent regression analyses.

```{r initiate}
# define alternative package directory
r_on_server <- T # MANUAL EDIT! ARE YOU ON LOCAL R SESSION OR USING ONE OF THE IMISE SERVERS (E.G. FOROSTAR)?
if (r_on_server == T) {
  computer <- "amanMRO" # MANUAL EDIT! SEE THE FOLDER RIGHT BELOW FOR THE APPROPRIATE PACKAGE FOLDERS
  .libPaths(paste0("/net/ifs1/san_projekte/projekte/genstat/07_programme/rpackages/", computer))
}

# Packages for efficiently loading data
for (i in c(
  "knitr",
  "data.table",
  "MASS",
  "ggplot2",
  "scales",
  "CarlHelpR",
  "toolboxH"
)) {
  suppressPackageStartupMessages(library(i, character.only = TRUE))
}
```

# Data simulation and setup

```{r load}
# load the simuation results
newest.results <- newest_file(look_for = "SimulationResultsForSTEP2", subfolder = "results", print_full = T)
all.scenarios <- fread(newest.results)
```

# Exploratory plots

There are a few descisions that I need to make:
* Bereiche or Quantile?
* Best number of categories?
* Performance compared with dichotomized glm?
* Performance compared with Rank correlation?
* Performance of ComBat
* Effect, the Inverse Normal Transformation (INT) has on the data/results

Our best guess was, that grouping the metabolites into more categories than just 2 would result in the retaining of information and thus statistical power. These plots are supposed to illustrate the comparison between dichotomization and different versions of the proportional odds approach.

For the proportional odds model, the number of categories (`categnum`) and the way the categories were established (`categart`) vary. In this simulation, `r unique(all.scenarios$categnum)` were the number of categories tested.

For the data simulation, the parameters were the percentage of inflated zero-values (`proz0`) and the effekt size (`effekt`) varied. For each scenario, 5000 sampels were drawn. Each scenario was computed 1000 times. 

## Dichotomization vs. proportional odds model

First it needs to be established, whether the proportional odds approach performs better than dichotomization. Intuitively it should, but we can test this by plotting the p-values of the approaches against each other and observe their properties depending on the different parameters. 

This first plot, plotting all p-values only shows, that the the percentage of zero-inflated values has a high impact on the distribution depending on the effect size. For small effect-sizes, which are the most interesting for our study, a distinction seems difficult. However, the plot let's us hypothesize, that that the proportional odds model outperforms the dichotomization model with increasing effect-size and a lower percentage of zero-inflated values. 

```{r question.1.1, results='markup'}
# First question: Dicho vs Logit
# plot all dicho vs. prop odds pvals
p1 <- ggplot(all.scenarios, aes(-log10(dicho.pval),
                                -log10(logit.pval),
                                col = factor(proz0),
                                pch = categart,
                                size = categnum,
                                alpha = rev(categnum))) + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1, lty = 2) + 
  facet_wrap(~effekt , scales = "free") + 
  scale_y_continuous(breaks = pretty_breaks(4)) +
  scale_x_continuous(breaks = pretty_breaks(4)) +
  scale_color_brewer(palette = "Set1") +
  theme_carl()

# plot dicho vs logit pvals (LARGE PLOT, NOT VERY ILLUSTRATIVE)
# p1
```

Taking the average p-values over all realizations provides a clearer picture for the differences for small effect-sizes. The proportional odds approach clearly outperforms dichotomization. Only when the zero-inflation is as high as 80%, the performances are similar. The table shows the sum of p-values for each approach <= 0.05 for each combination of number of categories and type of category.

```{r question.1.2, results='markup'}
# plot mean (over realizations) dicho vs. prop odds pvals
# über realisationen mitteln für Übersichtlichkeit
test.1 <- all.scenarios[, lapply(.SD, mean), by = .(effekt, proz0, categart, categnum, batch.effect)]

# same as p2 but with different symbols etc.
p2.5 <- ggplot(test.1, aes(-log10(dicho.pval),
                           -log10(logit.pval),
                           col = categnum,
                           pch = categart,
                           size = proz0,
                           alpha = rev(factor(proz0)))) +
  geom_point() +
  facet_wrap(~effekt, scales = "free") +
  scale_x_continuous(breaks = pretty_breaks(4)) +
  scale_y_continuous(breaks = pretty_breaks(4)) +
  geom_abline(intercept = 0, slope = 1, lty = 2) +
  scale_color_distiller(palette="Spectral") +
  scale_alpha_manual(values = c(0.3, 0.5, 0.7, 0.9)) + 
  theme_carl() +
  theme(text = element_text(size=15))

p2.5

# number of categories as deciding factor
kable(all.scenarios[ , lapply(.SD, function(x) sum(x <= 0.05)), by = .(categart,categnum),
                     .SDcols = c("linmod.pval.full.data",
                                 "linmod.pval.zero.inflated",
                                 "spearman.cor.pval.full.data",
                                 "spearman.cor.zero.inflated",
                                 "logit.pval",
                                 "dicho.pval",
                                 "spearman.cor.pval.zero.inflated.transformed",
                                 "linmod.beta.zero.inflated.transformed")])
```

## Linear model vs. proportional odds model

Additionally we look at the performance of the prop-odds model against a linear model approach. We can see that the approaches compare in Terms of Power. The loss of information in the proportional odds approach pit against the violation of the normality-assumption of the linear model approach. 

```{r question.1.3, results='markup'}
# plot mean (over realizations) linmod vs. prop odds pvals
# how does logit model compare to linmod?
p3 <- ggplot(test.1, aes(x = -log10(linmod.pval.zero.inflated),
                         y = -log10(logit.pval),
                         col = factor(proz0),
                         pch = categart,
                         size = categnum,
                         alpha = rev(categnum))) + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1, lty = 2) + 
  facet_wrap(~effekt , scales = "free") + 
  ggtitle(label = "P-value comparison", subtitle = "Zero-inflated data fit with proportional-odds model \nagainst zero-inflated data fit with linear model") +
  theme_carl()

# plot 3: -log10 pvalues proportional-odds model vs. linear model
p3

p3.5 <- ggplot(test.1, aes(x = linmod.beta.zero.inflated,
                           y = logit.beta,
                           col = factor(proz0),
                           pch = categart,
                           size = categnum,
                           alpha = rev(categnum))) + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1, lty = 2) + 
  facet_wrap(~effekt , scales = "free") + 
  ggtitle(label = "Beta comparison", subtitle = "Zero-inflated data fit with proportional-odds model \nagainst zero-inflated data fit with linear model") +
  theme_carl()

# plot 3.5: betas proportional-odds model vs. linear model
p3.5
```

## Optimum for number of categories

Plotting the p-values against the number of categories reveals a maximum towards a higher number of categories. However, the slope might be distorted due to the higher amount of different categories <10. Looking at the categories >10 only reveals no increased model performance when the number of categories exeeds 10. Again, the interpretation of the results for small effect sizes proves difficult, but the data suggests no increasded power upwards of 10 categories. 

```{r question.2, results='markup'}
# second question: what number of categories is optimal?
# plot -log10(Pwert) against number of categories
p4 <- ggplot(test.1,
             aes(x = categnum,
                 y = -log10(logit.pval),
                 pch = categart,
                 color = factor(proz0))) +
  facet_wrap(~ effekt, scale = "free") +
  geom_smooth(method = "loess", aes(group = paste(factor(proz0), categart), lty = categart), alpha = 0.2, fullrange=T, span = 1.5) +
  geom_point(size = 2.5) +
  scale_x_continuous(breaks = pretty_breaks(4)) +
  scale_y_continuous(breaks = pretty_breaks(4)) +
  scale_color_brewer(palette = "Set1") +
  theme_carl()

# all categnum
p4

# large categories
p4.1 <- ggplot(test.1[categnum %in% c(7,8,9,10,20,30,40,50),],
               aes(x = categnum,
                   y = -log10(logit.pval),
                   pch = categart,
                   color = factor(proz0))) +
  facet_wrap(~ effekt, scale = "free") +
  geom_smooth(method = "loess", aes(group = paste(factor(proz0), categart), lty = categart), alpha = 0.2, fullrange = T, span = 1.5) +
  geom_point(size = 2.5) +
  scale_x_continuous(breaks = pretty_breaks(4)) +
  scale_y_continuous(breaks = pretty_breaks(4)) +
  scale_color_brewer(palette = "Set1") +
  theme_carl()

# only look at higher number of categories
p4.1

# look at categories 2:10
p4.2 <- ggplot(test.1[categnum %in% c(2:10,20), ],
               aes(x = categnum,
                   y = -log10(logit.pval),
                   pch = categart,
                   color = factor(proz0))) +
  facet_wrap(~ effekt, scale = "free") +
  geom_smooth(method = "loess", aes(group = paste(factor(proz0), categart), lty = categart), alpha = 0.2, fullrange = T, span = 1.5) +
  geom_point(size = 2.5) +
  scale_x_continuous(breaks = pretty_breaks(4)) +
  scale_y_continuous(breaks = pretty_breaks(4)) +
  scale_color_brewer(palette = "Set1") +
  theme_carl()

# only look at higher number of categories
p4.2
```

## Categories based on quantiles or range

The previous plots suggest that the categories based on quantiles perform better in terms of mean p-value for each category, which is again dependent on effect size and percentage of zero-inflation. Plotting the, averaged over all realizations of the model, -log10 p-values against each other reveal a slight advantage of quantile-defined categories, at least for larger effect sizes. For the effect sizes of 0.01 and 0.02 in 80% zero inflated data, no conclusion can be drawn from the data. However, the general trend of the simulation suggests a slight advantage of quantiles over ranges for subdividing the data into categories  

```{r question.3, results='markup'}
# third question: Bereiche oder Quantile
# plot bereiche ~ quantile pvals
test.2 <- dcast.data.table(data = test.1, formula = effekt + proz0 + categnum + batch.effect ~ categart, value.var = "logit.pval")
p5 <- ggplot(test.2,
             aes(x = -log10(bereiche),
                 y = -log10(quantile),
                 color = categnum,
                 size = proz0,
                 pch = factor(batch.effect))) +
  facet_wrap(~effekt, scale = "free") + 
  geom_point(alpha = 1) +
  geom_abline(intercept = 0, slope = 1) + 
  scale_color_distiller(palette="Spectral") + 
  scale_alpha_manual(values = c(0.3,0.9))

p5
```

## Additional comparisons

Comparing the results of the, always fit with zero-inflated data, prop-odds model fit with the, either fit to the full or zero-inflated data set, rank correlation and linear model results The boxplots compare the p-value distribution of all approaches. Since we discontinue to look at the differences of area vs. quantiles as a measure for the categories, we'll continue with only the range-based categories. 

```{r question.4.1.prep, results='markup'}
# define a general plot function
q_4 <- function(x,y) {
  p4 <- ggplot(test.1[categart == "bereiche", ], aes(x = -log10(get(x)),
                                                     y = -log10(get(y)),
                                                     col = factor(proz0),
                                                     pch = factor(batch.effect),
                                                     size = categnum,
                                                     alpha = rev(categnum))) + 
    geom_point() +
    ylab(y) +
    xlab(x) +
    geom_abline(intercept = 0, slope = 1, lty = 2) +
    scale_color_brewer(palette="Set1") + 
    scale_alpha_continuous(range = c(0.3, 1)) +
    facet_wrap(~effekt , scales = "free") + 
    theme_carl() #+
    #theme(text = element_text(size=15))
  
  return(p4)
}

# when not looking at categorical data
q_5 <- function(x,y) {
  p5 <- ggplot(test.1[categart == "bereiche", ], aes(x = -log10(get(x)),
                                                     y = -log10(get(y)),
                                                     col = factor(effekt),
                                                     pch = factor(batch.effect),
                                                     size = proz0,
                                                     alpha = rev(proz0))) + 
    geom_point() +
    ylab(y) +
    xlab(x) +
    geom_abline(intercept = 0, slope = 1, lty = 2) +
    scale_color_brewer(palette="Dark2") + 
    scale_alpha_continuous(range = c(0.3, 1)) +
    facet_wrap(~effekt , scales = "free") + 
    theme_carl() +
    theme(text = element_text(size=15))
  
  return(p5)
}

```

### Linear model and Spearman's correlation

First we check what influence the zero-inflation has on the linear model and spearman correlation power. We plot p-values of both approaches with the full and the zero-inflated data against each other. We can see that the the linear model sufferes more from the loss of data even at 20% of zero-values, while the rank correlation starts to suffer visibly at 50% zero-inflation.

```{r question.4.0}
p5.0.1 <- q_5(x = "linmod.pval.full.data", y = "linmod.pval.zero.inflated")
p5.0.1 +
  guides(color = F,
         # alpha = guide_legend(title=NULL),
         size = guide_legend(title="Zero-Inflation [%]"),
         pch = guide_legend(title="Batch effect")) + 
  scale_shape_discrete(labels = c("No", "Yes")) + 
  scale_alpha_continuous(guide = F, range = c(0.3, 1)) +
  scale_color_brewer(palette="Dark2") + 
  xlab(label = expression('-log'[10]*' P-values full data model fit')) +
  ylab(label = expression('-log'[10]*' P-values zero-inflated data model fit')) +
  theme(legend.position="top")

  

p5.0.2 <- q_5(x = "spearman.cor.pval.full.data", y = "spearman.cor.pval.zero.inflated")
p5.0.2
```

Comparing between approaches shows the expected advantage of a linear model over rank correlation for the full data set. Zero-inflating the data evens the performance out. The violated normality assumption of the linear model should be considered when choosing between the approaches. A non-parametric approach like Spearman's rank correlation seems like the choice of reason. 

```{r question.4.1}
p5.1.2 <- q_5(x = "spearman.cor.pval.full.data", y = "linmod.pval.full.data")
p5.1.2

p5.1.1 <- q_5(x = "spearman.cor.pval.zero.inflated", y = "linmod.pval.zero.inflated")
p5.1.1
```

### Linear model and proportional odds model

While the optimal approach of fitting a linear model with the full data set remains superior to the prorportional odds model with zero-inflated data, the advantage all but disappears when fitting the linear model to the zero-inflated data. A small number of categories (<10), which leads to an increased loss of information for the proportional odds model visibly hurts the models performance. Otherwise, zero-inflation creates a level playing field for both models.

```{r question.4.2}
p4.2.1 <- q_4(x = "linmod.pval.full.data", y = "logit.pval")
p4.2.1 + ggtitle(label = "-log10 P-values linmod vs prop-odds",
                 subtitle = "Comparison of p-values of linear model fit to normally distributed vs.\nproportional-odds model fit to zero-inflated data")

p4.2.2 <- q_4(x = "linmod.pval.zero.inflated", y = "logit.pval")
p4.2.2 + ggtitle(label = "-log10 P-values linmod vs prop-odds",
                 subtitle = "Comparison of p-values of linear vs. proportional-odds model both fit to zero-inflated data")
```

### Spearman's correlation and proportional odds model

It is to be expected that Spearman's correlation of the full data set outperforms the zero-inflated proportional odds approach. Setting both approaches to the zero-inflated data against each other again only reveals an advantage of Spearman's correlation coefficient over a proportional odds model with very few (<10) categories. Otherwise the models perform equally.

```{r question.4.3}
p4.3.1 <- q_4(x = "spearman.cor.pval.full.data", y = "logit.pval")
p4.3.1

p4.3.2 <- q_4(x = "spearman.cor.pval.zero.inflated", y = "logit.pval")
p4.3.2
```

```{r question.4.4, include = F, eval = F}
# boxplot
test.3 <- melt(data = all.scenarios,
               id.vars = c("effekt", "proz0", "categart", "categnum", "batch.effect"),
               measure.vars = c("logit.pval",
                                "dicho.pval",
                                "linmod.pval.zero.inflated",
                                "linmod.pval.full.data",
                                "spearman.cor.pval.zero.inflated",
                                "spearman.cor.pval.full.data",
                                "spearman.cor.pval.zero.inflated.transformed",
                                "linmod.pval.zero.inflated.transformed",
                                "linmod.pval.zero.inflated.transformed.rescaled"))

# outer loop for proz0
lapply(unique(all.scenarios$proz0), function(y){
  
  # inner loop for effect size
  lapply(unique(all.scenarios$effekt), function(x){
    
    p7 <- ggplot(test.3[effekt == x & proz0 == y, ],
                 aes(y = -log10(value),
                     x = as.factor(categnum),
                     fill = variable)) +
      geom_boxplot() + 
      coord_flip() +
      facet_wrap( ~ categart) + 
      ggtitle(label = "Boxplot of Pvalues",
              subtitle = paste0("Effect-size = ", x, " and zero-inflation by ", y, "%")) +
      theme_minimal() +
      scale_fill_brewer(palette = "Spectral")
    
    p7
  })
})
```

## Comparison of effect sizes between linear model and proportional odds model

Plotting the average effect sizes from the linear model and the prop-odds model reveal a drastic overestimation of effect size with increasing effect size in the data by the prop-odds model. Additionally, the zero-inflation seems to be influencig seems to be adding to to this overestimation while leading to an underestimation of the added effect in the linear model.

### Plot definition

```{r question.5}
# plot without -log10
p_8 <- function(x,y) {
  
  # define
  p8 <- ggplot(test.1[categart == "bereiche",],
               aes(x = get(x),
                   y = get(y),
                   size = proz0,
                   col = factor(effekt),
                   pch = factor(batch.effect),
                   alpha = rev(factor(proz0)))) +
    geom_point() +
    geom_abline(intercept = 0, slope = 1, lty = 2, col = "black") +
    # facet_wrap( ~ , scales = "fixed") +
    scale_color_brewer(palette = "Dark2") +
    ylab(y) +
    xlab(x) +
    scale_x_continuous(breaks = pretty_breaks(5)) +
    scale_y_continuous(breaks = pretty_breaks(5)) +
    scale_alpha_manual(values = c(0.3, 0.5, 0.7, 0.9)) + 
    theme_carl()# +
    #theme(text = element_text(size=18))
  
  # plot
  p8
}

# plot with -log10 of x & y
p_9 <- function(x,y) {
  
  # define
  p_9 <- ggplot(test.1[categart == "bereiche",],
                aes(x = -log10(get(x)),
                    y = -log10(get(y)),
                    size = proz0,
                    col = factor(effekt),
                    pch = factor(batch.effect),
                    alpha = rev(factor(proz0)))) +
    geom_point() +
    geom_abline(intercept = 0, slope = 1, lty = 2, col = "black") +
    # facet_wrap( ~ , scales = "fixed") +
    scale_color_brewer(palette = "Dark2") +
    ylab(y) +
    xlab(x) +
    scale_x_continuous(breaks = pretty_breaks(5)) +
    scale_y_continuous(breaks = pretty_breaks(5)) +
    scale_alpha_manual(values = c(0.3, 0.5, 0.7, 0.9)) + 
    theme_carl() #+
    #theme(text = element_text(size=18))
  
  # plot
  p_9
}
```

### Effect-size plots - prop.-odds

The effect size/beta of a proportional odds model is not equivalent to the effect size of a linear model. I can't find the conversion factor however. 

```{r q.5.plots}
# with facet wrap
p8.1 <- p_8(x = "linmod.beta.zero.inflated",
            y = "logit.beta") + 
  ggtitle(label = "Beta estimate comparison", subtitle = "Zero-infalted prop-odds vs.\nzero-inflated data fit with lm")

# split for effect size
p8.1 + facet_wrap(~effekt , scales = "free")

# in single plot
p8.1

# define effect-size plot with zero-inflated linmod data
p8.2 <- p_8(x = "linmod.beta.full.data",
            y = "logit.beta") + 
  ggtitle(label = "Beta estimate comparison", subtitle = "Zero-inflated prop-odds vs.\n full data fit with lm")

# single plot
p8.2

# facetted plot
p8.2  + facet_wrap(~effekt , scales = "free")
```

## Comparison of inverse-normal transformed zero-inflated data fit in lm()

### Effect-size plots

Check performance of inverse-normal transformed data against categorical and regular zero-inflated data. During the transformation the data is scaled to SD = 1. Rescaling them fixes the wrongly estimated meta for the non-zero-inflated INT transformed data

```{r int.betas}
# beta zero inflated lm vs. full lm
p_8(x = "linmod.beta.zero.inflated", y = "linmod.beta.full.data")

# facetted
p_8(x = "linmod.beta.zero.inflated", y = "linmod.beta.full.data") +
  facet_wrap(~effekt , scales = "free") +
  ggtitle(label = "Beta estimate comparison", subtitle = "Zero-inflated data fit to linear model vs.\n full data fit with linear model")

# betas of INT zero inflated data vs. full lm
p_8(x = "linmod.beta.zero.inflated.transformed", y = "linmod.beta.full.data") +
  facet_wrap(~effekt , scales = "free") +
  ggtitle(label = "Beta estimate comparison", subtitle = "Zero-inflated and INT data fit to linear model vs.\n full data fit with linear model")

# betas of rescaled INT zero inflated data vs. full lm
p_8(x = "linmod.beta.zero.inflated.transformed.rescaled", y = "linmod.beta.full.data") +
  facet_wrap(~effekt , scales = "free") +
  ggtitle(label = "Beta estimate comparison", subtitle = "Zero-inflated and INT and rescaled data fit to linear model vs.\n full data fit with linear model")

# effect sizes
p_8(x = "linmod.beta.zero.inflated.transformed.rescaled", y = "linmod.beta.zero.inflated") +
  # xlim(c(0,0.4)) +
  # ylim(c(0,0.4)) +
  geom_vline(xintercept = c(0,0.01,0.02,0.05,0.1,0.3)) +
  geom_hline(yintercept = c(0,0.01,0.02,0.05,0.1,0.3)) +
  ggtitle(label = "Beta comparison",
          subtitle = "zero-inflated data ~ zero-inflated IN transformed and rescaled data")

p_8(x = "linmod.beta.zero.inflated.transformed.rescaled", y = "linmod.beta.full.data") +
  xlim(c(0,0.4)) +
  ylim(c(0,0.4)) +
  geom_vline(xintercept = c(0,0.01,0.02,0.05,0.1,0.3)) +
  geom_hline(yintercept = c(0,0.01,0.02,0.05,0.1,0.3)) +
  ggtitle(label = "Beta comparison",
          subtitle = "Full data ~ zero-inflated IN transformed and rescaled data")
```

### P-value plots

compare the -log10 p-values of the regular lm fit to full data, zero-inflated and INT data.

```{r question.6}
# Comparison with full data
p_9(x = "linmod.pval.full.data", y = "linmod.pval.zero.inflated") + facet_wrap(~effekt , scales = "free") +
  ggtitle(label = "P-value comparison",
          subtitle = "Full data ~ zero-inflated untransformed data")

p_9(x = "linmod.pval.full.data", y = "linmod.pval.zero.inflated.transformed") + facet_wrap(~effekt , scales = "free") +
  ggtitle(label = "P-value comparison",
          subtitle = "Full data ~ zero-inflated IN-transformed data")

p_9(x = "linmod.pval.full.data", y = "linmod.pval.zero.inflated.transformed.rescaled") + facet_wrap(~effekt , scales = "free") +
  ggtitle(label = "P-value comparison",
          subtitle = "Full data ~ zero-inflated IN-transformed and rescaled data")

# comparison zero inflated untransformed vs transformed
p_9(x = "linmod.pval.zero.inflated", y = "linmod.pval.zero.inflated.transformed.rescaled") + facet_wrap(~effekt , scales = "free") +
  ggtitle(label = "P-value comparison",
          subtitle = "zero-inflated untransformed data ~ zero-inflated IN-transformed data")

# INT zero-inflated data vs spearman cor. of untransformed data
p_9(x = "spearman.cor.pval.zero.inflated", y = "linmod.pval.zero.inflated.transformed.rescaled") + facet_wrap(~effekt , scales = "free") +
  ggtitle(label = "P-value comparison",
          subtitle = "zero-inflated untransformed ~ zero-inflated IN-transformed and rescaled data")
```

### QQ plots of batch-effect ANOVA pre- and post-ComBat

#### For the mean data

```{r qq.plots.anova}
# pvals, beta, r² of transformed data based on batch 1/0
qq_conf(test.1$p.ANOVA.full.pre.ComBat, main = "QQplot full data before ComBat")
qq_conf(test.1$p.ANOVA.full.post.ComBat, main = "QQplot full data after ComBat")

# for zero-inflated data
qq_conf(test.1$p.ANOVA.zero.infl.pre.ComBat, main = "QQplot zero-inflated data before ComBat")
qq_conf(test.1$p.ANOVA.zero.infl.post.ComBat, main = "QQplot zero-inflated data after ComBat")

# for transformed data
qq_conf(test.1$p.ANOVA.zero.infl.post.ComBat.post.transformation, main = "QQplot zero-inflated & transformed data post ComBat")
```

#### For one example block w/ & w/o batch effects

```{r qq.plots.anove.example}
# check anova transformed
# full data
# with out 
qq_conf(all.scenarios[combat.run == 1, p.ANOVA.full.pre.ComBat],
        main = "QQ plot of ANOVA p-vals on full data\nwithout batch effects pre-ComBat")
qq_conf(all.scenarios[combat.run == 1, p.ANOVA.full.post.ComBat],
        main = "QQ plot of full data without batch effects pre-ComBat")

# with batch effect
qq_conf(all.scenarios[combat.run == 251, p.ANOVA.full.pre.ComBat],
        main = "QQ plot of full data with batch effects pre-ComBat")
qq_conf(all.scenarios[combat.run == 251, p.ANOVA.full.post.ComBat],
        main = "QQ plot of full data with batch effects post-ComBat")

pdf("180717_dataSimulationComBatINTBatchEffects.pdf", width = 10, height = 7)
par(mfrow = c(2,3))

# without batch effect
qq_conf(all.scenarios[combat.run == 1, p.ANOVA.zero.infl.pre.ComBat],
        main = "QQ plot of zero-inflated data\nwithout batch effects pre-ComBat")
qq_conf(all.scenarios[combat.run == 1, p.ANOVA.zero.infl.post.ComBat],
        main = "QQ plot of zero-inflated data\nwithout batch effects post-ComBat")
qq_conf(all.scenarios[combat.run == 1, p.ANOVA.zero.infl.post.ComBat.post.transformation],
        main = "QQ plot of zero-inflated data without batch\neffects post-ComBat after INT")

# with batch-effect
qq_conf(all.scenarios[combat.run == 251, p.ANOVA.zero.infl.pre.ComBat],
        main = "QQ plot of zero-inflated data\nwith batch effects pre-ComBat")
qq_conf(all.scenarios[combat.run == 251, p.ANOVA.zero.infl.post.ComBat],
        main = "QQ plot of zero-inflated data\nwith batch effects post-ComBat")
qq_conf(all.scenarios[combat.run == 251, p.ANOVA.zero.infl.post.ComBat.post.transformation],
        main = "QQ plot of zero-inflated data with batch\neffects post-ComBat after INT")

dev.off()
```

# weird associations after INT

```{r int.batch.problem, eval = F, include = F}
# !!!
# first get the combat.data from STEP1 for combat run 251
# !!!

fit_plots <- function(combat.data, pdf_name){
# ComBated data
pdf(pdf_name, width = 14)
par(mfrow=c(1,2))
for (i in c(81, 85, 89, 93)) {
  
  # very ugly ad hoc indexing
  my.zero.infl <- c(0,20,50,80)
  dummy.index <- c(81, 85, 89, 93)
  my.dummy.index <- which(i == dummy.index)
  
  # set indices
  my.y <- paste0("y.0infl.", i)
  my.x <- paste0("x.", i)
  my.y.tr <- paste0("y.0infl.", i, ".transformed.rescaled")
  
  # fit lm
  lm <- lm(get(my.y) ~ get(my.x), data = combat.data)
  
  # coeffs for lm
  rmse <- round(sqrt(mean(resid(lm)^2)), 2)
  coefs <- coef(lm)
  b0 <- round(coefs[1], 2)
  b1 <- round(coefs[2],2)
  r2 <- round(summary(lm)$r.squared, 2)
  
  # Now build up the equation using constructs described in ?plotmath:
  eqn <- bquote(italic(beta) == .(b1) * "," ~~ 
                  r^2 == .(r2) * "," ~~ RMSE == .(rmse))
  
  
  lm.tr<- lm(get(my.y.tr) ~ get(my.x), data = combat.data)
  
  # coeffs for lm
  rmse.tr <- round(sqrt(mean(resid(lm.tr)^2)), 2)
  coefs.tr <- coef(lm.tr)
  b0.tr <- round(coefs.tr[1], 2)
  b1.tr <- round(coefs.tr[2],2)
  r2.tr <- round(summary(lm.tr)$r.squared, 2)
  
  # Now build up the equation using constructs described in ?plotmath:
  eqn.tr <- bquote(italic(beta) == .(b1.tr) * "," ~~ 
                  r^2 == .(r2.tr) * "," ~~ RMSE == .(rmse.tr))
  
  # set range
  my.range <- combat.data[, range(.SD), .SDcols = my.y]
  my.range.diff <- c(0,1,1,1)
  my.range[1] <- my.range[1] - my.range.diff[my.dummy.index]
  
  # untransformed
  combat.data[ , plot(get(my.x), get(my.y),
                      main = paste0("Untransformed, ",
                                    my.zero.infl[my.dummy.index],
                                    "% zero-inflated data"),
                      sub = eqn,
                      xlab = "X",
                      ylab = paste0("Y, ", my.zero.infl[my.dummy.index],"% zero-inflated"),
                      ylim = my.range,
                      pch = 20,
                      col = "grey40")]
  combat.data[ , abline(lm, lwd = 3, col = "red")]

  # transformed
  combat.data[ , plot(get(my.x), get(my.y.tr), main = paste0("INT, ",
                                                          my.zero.infl[my.dummy.index],
                                                          "% zero-inflated data"),
                      sub = eqn.tr,
                      xlab = "X",
                      ylab = paste0("Y, ", my.zero.infl[my.dummy.index],"% zero-inflated"),
                      ylim = my.range,
                      pch = 20,
                      col = "grey40")]
  
  combat.data[ , abline(lm.tr, lwd = 3, col = "red")]

}
par(mfrow=c(1,1))
dev.off()
}

# create the data for combat run 251 twice! NEEDS TO BE DONE MANUALLY
# with combat and transformation
fit_plots(combat.data = combat.data, pdf_name = "180717_zero_inflated_fit_AFTER_ComBat")

# without ComBat but including the transformation (omit ComBat manually here -> see effect of INT on zeros)
fit_plots(combat.data = combat.data, pdf_name = "180717_zero_inflated_fit_NO_ComBat")

```


```{r save}
# write.table(x = all.scenarios,
#             file = paste0(pathwd, "/results/",
#                           format(Sys.time(), '%y%m%d'),
#                           "_SimulationResults.csv"),
#             sep = "\t", row.names = F)
```

```{r SessionInfo, echo=F, results='markup'}
sessionInfo()
```            