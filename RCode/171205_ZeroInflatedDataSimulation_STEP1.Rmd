---
output:
  html_document:
    toc: true
    number_sections: false
    toc_depth: 3
author: "Carl Beuchel"
date: "`r format(Sys.time(), '%d %B, %Y')`"
---

```{r knitr, cache = F, results = "markup", echo = T, warning = T}
gc()

# Set the global knitr options
knitr::opts_chunk$set(cache = F, results = "hide", echo = T ,include = T)

# what should the output file be called?
filename = "STEP 1: Zero-inflated data simulation"
```

# `r filename`
***
This script is a small data simulation to help figure out an appropriate data transformation withouth loosing too much statistical power in the subsequent regression analyses.

```{r initiate, cache = F, results = "hide", echo = F}
# choose correct working directory
r_on_server <- F

# number of cores used in parallel processing
if (r_on_server == T) {
  .libPaths("/net/ifs1/san_projekte/projekte/genstat/07_programme/rpackages/forostar/")
  number.of.cores <- 10
} else if (r_on_server == F) {
  number.of.cores <- parallel::detectCores()/2
}

# Packages
for (i in c(
  "knitr",
  "data.table",
  "broom",
  "ordinal",
  "MASS",
  "parallel",
  "CarlHelpR",
  "toolboxH",
  "here",
  "sva"
)) {
  suppressPackageStartupMessages(library(i, character.only = TRUE))
}
```

## Data simulation and setup

```{r param.setup}
# code by HKirsten (holgerman)
# set parameters for simulation, e.g. effect size, n...
set.seed(0815)

# magnitude of simulated effect  
effekte = c(0.01, 0.02, 0.05, 0.1, 0.3)

# size of simulated data set
n = 3000

# magnitude of zero-inflation in %
proz0 = c(20, 50, 80)

# how often should each step be repeated?
realisierungen = 1000

# type of categories
categart = c("quantile", "bereiche")

# number of categories
categnum = c(2:10, 20, 30, 40, 50)

# add a batch to test the effect of ComBat on badly distributed data
batch <- c(0, 0.05, 0.1)
```

## Create scenarios

```{r create.scenarios}
# create dt with all scenario combinations
szenarien <- data.table(expand.grid(effekte = effekte,
                                    n = n,
                                    proz0 = proz0,
                                    categart = categart,
                                    categnum = categnum,
                                    realization = 1:realisierungen))
szenarien[,num := 1:.N]
```

## Create data and run lm

```{r test.data}
# apply over each szenario
res <- mclapply(szenarien$num,  function(i) {
  
  # i <- 268 # debug
  my.x <- szenarien[num == i]
  
  # reassign the effects for readability
  effekt <- my.x$effekte
  n <- my.x$n
  proz0 <- my.x$proz0
  categart <- my.x$categart
  categnum <- my.x$categnum
  num <- my.x$num 
  batch <- my.x$batch
  real <-  my.x$realization
  
  # create data for scenario
  df <- data.table(x = rnorm(n))
  df[, y :=  (effekt * x) + ((1 - effekt) * rnorm(n)) + 3]
  
  # create uncorrelated data to test for false positives
  df.control <- data.table::data.table(a = rnorm(n),
                                       b = rnorm(n))
  
  # add batch effects do df & control
  df[, batch := rep_len(letters[1:length(batch)], length.out = n)]
  for(my.batch in letters[1:length(batch)]) {
    df[y == (my.batch), y := y + (batch[which(letters == my.batch)] * rnorm(n))]
    df.control[a == (my.batch), a := a + (batch[which(letters == my.batch)] * rnorm(n))]
  }
  # fit lm
  linmod.full.data <- df[, summary(lm(y ~ x))]
  
  # fit uncorrelated model
  linmod.control <- df.control[, summary(lm(a ~ b))]
  
  # add spearman rank correlation to compare with Theis
  cor.full.data <- tidy(df[, cor.test(x = x, y = y, method = "spearman")])
  
  # correlate control data
  cor.control <- tidy(df.control[, cor.test(x = a, y = b, method = "spearman")])
  
  # add summary statistics
  res <- data.table(num = num,
                    realization = real,
                    effekt = effekt,
                    n = n,
                    proz0 = proz0,
                    categart = categart,
                    categnum = categnum,
                    spearman.cor.full.data = cor.full.data$estimate,
                    spearman.cor.pval.full.data = cor.full.data$p.value,
                    linmod.r2.full.data = linmod.full.data$r.squared,
                    linmod.pval.full.data = linmod.full.data$coefficients["x","Pr(>|t|)"],
                    linmod.beta.full.data = linmod.full.data$coefficients["x","Estimate"],
                    spearman.cor.control = cor.control$estimate,
                    spearman.cor.control = cor.control$p.value,
                    linmod.r2.control = linmod.control$r.squared,
                    linmod.pval.control = linmod.control$coefficients["b","Pr(>|t|)"],
                    linmod.beta.control = linmod.control$coefficients["b","Estimate"])
  
  #-----------------------------------------------------------------
  # create zero-inflated y-derived data
  # way one:
  df[, y.0infl := y]
  df[y < quantile(y.0infl, probs = proz0/100), y.0infl := 0]
  min.found = df[y.0infl > 0, min(y.0infl)]
  df[y.0infl > 0, y.0infl := y.0infl - 0.99*min.found]
  
  # alternatively, sample (at n = proz.0) from the distribution and set those sampled values == 0
  df[, y.0ifl.2 := y]
  df[sample(x = 1:n, size = n * proz0/100, replace = F), y.0ifl.2 := 0]
  #-----------------------------------------------------------------
  
  #-----------------------------------------------------------------
  # zero-inflate the control
  df.control[, a.0infl := a]
  df.control[a < quantile(a.0infl, probs = proz0/100), a.0infl := 0]
  min.found = df.control[a.0infl > 0, min(a.0infl)]
  df.control[a.0infl > 0, a.0infl := a.0infl - 0.99*min.found]
  
  df.control[, a.0infl.2 := a]
  df[sample(x = 1:n, size = n * proz0/100, replace = F), a.0infl.2 := 0]
  #-----------------------------------------------------------------
  
  # check
  # df[,.N,y.0infl==0]
  # hist(df$y.0infl, breaks = 50)
  
  #-----------------------------------------------------------------
  # do ComBat batch adjustment
  
  
  #-----------------------------------------------------------------
  
  # add lm and spearman-cor for zero-inflated data
  # fit lm
  linmod.zero.inflated <- df[, summary(lm(y.0infl ~ x))]
  
  # control
  linmod.control.zero.inflated <- df.control[, summary(lm(a.0infl ~ b))]
  
  # add spearman rank correlation to compare with Theis
  cor.zero.inflated <- tidy(df[, cor.test(x = x, y = y.0infl, method = "spearman")])
  
  # corr control
  cor.control.zero.inflated <- tidy(df.control[, cor.test(x = b, y = a.0infl, method = "spearman")])
  
  # enter results
  res[, `:=`(spearman.cor.zero.inflated = cor.zero.inflated$estimate,
             spearman.cor.pval.zero.inflated = cor.zero.inflated$p.value,
             linmod.r2.zero.inflated = linmod.zero.inflated$r.squared,
             linmod.pval.zero.inflated = linmod.zero.inflated$coefficients["x","Pr(>|t|)"],
             linmod.beta.zero.inflated = linmod.zero.inflated$coefficients["x","Estimate"],
             spearman.cor.control.zero.inflated = cor.control.zero.inflated$estimate,
             spearman.cor.control.pval.zero.inflated = cor.control.zero.inflated$p.value,
             linmod.control.r2.zero.inflated = linmod.control.zero.inflated$r.squared,
             linmod.control.pval.zero.inflated = linmod.control.zero.inflated$coefficients["b","Pr(>|t|)"],
             linmod.control.beta.zero.inflated = linmod.control.zero.inflated$coefficients["b","Estimate"])]
  
  # sort by y (category)
  setorder(df, y)
  
  # create categories to calculate polr/ordinal over
  if(categart == "quantile") {
    
    # categorisation over quantiles
    df[y.0infl > 0, y.0infl.categ := cut(y.0infl, breaks = quantile(y.0infl, probs = seq(0, 1, 1/categnum)), include.lowest = T)]
    categs.created <- df[, levels(y.0infl.categ)]
    df[y.0infl == 0, y.0infl.categ := factor(0, labels =  "[0]")]
    df[, y.0infl.categ := factor(y.0infl.categ, levels = c("[0]", categs.created))]
  } else if(categart == "bereiche"){
    
    # category separating over range
    df[y.0infl > 0, y.0infl.categ := cut(y.0infl, breaks = seq(0, max(y.0infl), length.out = categnum + 1), include.lowest = T)]
    categs.created <- df[, levels(y.0infl.categ)]
    df[y.0infl == 0, y.0infl.categ := factor(0, labels =  "[0]")]
    df[, y.0infl.categ := factor(y.0infl.categ, levels = c("[0]", categs.created))]
    
  } else {
    stop("Error")
  }
  
  # now we have the complete data and start fitting the model
  # start with fitting the regular glm for a dichotomized y
  linmod.dicho <- df[, summary(glm(y.0infl > 0 ~ x, family = "binomial"))]
  linmod.dicho.coefs <- setDT(as.data.frame(coefficients(linmod.dicho)), keep.rownames = T)
  linmod.dicho.pval <- unlist(linmod.dicho.coefs[rn == "x", "Pr(>|z|)"])
  linmod.dicho.beta <- unlist(linmod.dicho.coefs[rn == "x","Estimate"])
  
  # enter results
  res[, `:=`(dicho.pval = linmod.dicho.pval,
             dicho.beta = linmod.dicho.beta,
             dicho.AIC = linmod.dicho$aic)]
  
  # fit the ordinal proportional odds model
  logit.mod <- df[, summary(ordinal::clm(y.0infl.categ ~ x))]
  logit.mod.2 <- df[, ordinal::clm(y.0infl.categ ~ x)]
  logit.coefs <- setDT(as.data.frame(logit.mod$coefficients), keep.rownames = T)
  logit.pval <- unlist(logit.coefs[rn == "x", "Pr(>|z|)"])
  
  # enter results
  res[, `:=`(logit.beta = logit.mod.2$beta,
             logit.pval = logit.pval,
             logit.AIC = as.numeric(logit.mod.2$info$AIC))]
  
  return(res)
}, mc.cores = number.of.cores, mc.cleanup = T)

# bind everything together
all.scenarios <- rbindlist(res)

```

```{r save}
# Save for use in STEP2
save_csv_carl(file = all.scenarios,
              file_name = "SimulationResultsForSTEP2",
              subfolder = "results")
```

```{r SessionInfo, echo=F, results='markup'}
sessionInfo()
```            